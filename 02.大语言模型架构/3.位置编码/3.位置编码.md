# 3. 位置编码

## 1. 位置编码

不同于 RNN、CNN 等具有天然顺序建模能力的模型，**Transformer 中的自注意力机制（Self-Attention）本身是排列不变的（permutation-equivariant）**，即它无法感知输入序列中 Token 的顺序。因此，**必须显式地引入位置信息**，否则模型将无法区分不同位置的 Token。

为此，研究人员提出了两大类解决方案：

1. **绝对位置编码（Absolute Positional Encoding）**：将每个位置 $k$ 对应的位置向量直接加到输入嵌入中；
2. **相对位置编码（Relative Positional Encoding）**：在计算注意力时，显式建模 Query 和 Key 之间的相对距离。

---

## 1.1 绝对位置编码

绝对位置编码的基本思想是：为每个位置 $k$ 分配一个固定的或可学习的向量 $p_k$，然后将其加到输入向量 $x_k$ 上，形成最终输入：

$$
\tilde{x}_k = x_k + p_k
$$

其中 $p_k$ 仅依赖于位置 $k$，不依赖于内容。

### （1）训练式（Learnable Position Embeddings）

最简单的方式是将位置编码作为**可学习参数**。例如，若最大序列长度为 512，隐藏维度为 768，则初始化一个形状为 $(512, 768)$ 的矩阵作为位置嵌入表，在训练过程中与其他参数一同优化。

**优点**：
- 实现简单，易于集成；
- 可以拟合任意复杂的位置模式。

**缺点**：
- **缺乏外推性（extrapolation ability）**：训练时最大长度为 512，则推理时超过 512 的位置没有对应参数，无法直接使用；
- 虽然可以通过插值、外推初始化等方式缓解，但性能通常下降明显。

> **说明**：尽管存在改进方法（如层次分解），但标准训练式位置编码确实不具备自然外推能力，仍是其显著局限。

### （2）三角函数式（Sinusoidal Position Encoding）

由《Attention Is All You Need》提出，使用正弦和余弦函数生成位置编码：

$$
\begin{cases}
p_{k,2i} = \sin\left(\dfrac{k}{10000^{2i/d}}\right) \\
p_{k,2i+1} = \cos\left(\dfrac{k}{10000^{2i/d}}\right)
\end{cases}
$$

其中 $d$ 是位置编码维度，$i$ 是维度索引。

**优点**：
- 无需训练，可预先计算；
- 具备一定的**外推潜力**，因为函数定义域无限；
- 利用三角恒等式，理论上可以表达相对位置信息。

**缺点**：
- 实际外推效果受限于周期性和数值稳定性；
- 在长序列中可能出现“位置混淆”（高频震荡导致相似编码）。

> **说明**：虽然许多基础模型（如 BERT、ViT）使用可学习位置编码，但 Sinusoidal 仍广泛用于无需微调的场景（如生成模型、跨模态任务）。

### （3）递归式（Recurrent/Dynamical Position Encoding）

利用递归结构生成位置编码，例如通过 RNN 或微分方程建模：

$$
p_0 = \text{init},\quad p_{k+1} = f(p_k; \theta)
$$

ICML 2020 论文《Learning to Encode Position for Transformer with Continuous Dynamical Model》提出的 **FLOATER** 模型采用神经微分方程（Neural ODE）：

$$
\frac{dp_t}{dt} = h(p_t, t; \theta)
$$

**优点**：
- 具有良好的外推性（ODE 可积分至任意时间）；
- 更灵活，能建模非线性位置动态。

**缺点**：
- 计算成本高，训练复杂；
- 并行性差，影响推理速度。

> **说明**：RNN 虽有顺序处理机制，但其对位置的感知是隐式的、弱的，尤其在深层网络中容易丢失位置信息。因此，RNN+Transformer 混合结构仍可能需要位置编码。

### （4）相乘式（Multiplicative Position Encoding）

将位置信息以乘法方式融合：

$$
\tilde{x}_k = x_k \odot p_k
$$

或更复杂的交互形式（如双线性）。

**动机**：
- 加法可能被残差连接稀释；
- 乘法可实现“调制”作用，增强位置敏感性。

**现状**：
- 尚无广泛验证的优势；
- 多见于实验性研究（如中文模型探索）。

> **说明**：目前主流仍以加法为主，乘法尚未成为标准做法。

---

## 1.2 相对位置编码

相对位置编码的核心思想是：**在计算注意力权重时，显式建模 Query 与 Key 之间的相对距离 $i - j$**，而非为每个位置分配绝对坐标。

### （1）经典式（Relative Position Representations, RPR）

出自 Google 论文《Self-Attention with Relative Position Representations》（2018）。

原始注意力得分展开后包含四项。RPR 的改进在于：

- 去掉 $p_i W_Q$ 项；
- 将 $p_j W_K$ 替换为依赖于相对位置 $i-j$ 的向量 $R^K_{i,j}$；
- $p_j W_V$ 替换为 $R^V_{i,j}$。

最终得：

$$
a_{i,j} = \text{softmax}\left( q_i k_j^T + q_i (R^K_{i,j})^T \right),\quad o_i = \sum_j a_{i,j} (v_j + R^V_{i,j})
$$

其中：
$$
R^K_{i,j} = P_K[\text{clip}(i - j, p_{\min}, p_{\max})]
$$
$P_K$ 是可学习的相对位置嵌入表，长度有限，通过 clip 截断实现任意长度外推。

**优点**：
- 天然支持长序列；
- 更符合语言中相对位置的重要性。

**缺点**：
- 引入额外参数；
- clip 边界可能造成不连续。

### （2）XLNet式（Transformer-XL Position Encoding）

出自《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》。

核心思想是对完整展开的注意力项进行重参数化：

$$
\text{score}_{i,j} = x_i W_Q W_K^T x_j^T + x_i W_Q W_{K,R}^T R_{i-j}^T + u W_K^T x_j^T + v W_{K,R}^T R_{i-j}^T
$$

其中 $R_{i-j}$ 使用 Sinusoidal 编码，$u, v$ 为可学习向量。

**特点**：
- 结合了绝对与相对思想；
- 支持长上下文（segment-level recurrence）；
- 位置信息仅作用于注意力，不加入 $v_j$。

### （3）T5式（T5 Relative Position Bias）

出自《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》。

T5 的做法更简洁：**完全移除显式位置嵌入**，仅在注意力得分上添加一个可学习的偏置 $\beta_{i,j}$：

$$
\text{score}_{i,j} = q_i k_j^T + \beta_{i,j}
$$

其中 $\beta_{i,j} = B[f(i-j)]$，$f(\cdot)$ 是“分桶函数”：

| $i-j$     | 0  | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8~11 | 12~19 | ≥20 |
|-----------|----|----|----|----|----|----|----|----|------|-------|-----|
| $f(i-j)$  | 0  | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8    | 9     | 10  |

**优点**：
- 极简设计，参数少；
- 分桶使远距离位置共享编码，提升泛化；
- 支持任意长度。

### （4）DeBERTa式（Disentangled Attention）

出自《DeBERTa: Decoding-enhanced BERT with Disentangled Attention》。

将内容和位置信息解耦：

- 内容向量：$q_i^c = x_i W_Q$, $k_j^c = x_j W_K$
- 位置向量：$k_{i,j}^p = R_{i,j} W_K$, $q_{j,i}^p = R_{j,i} W_Q$

注意力得分：

$$
\text{score}_{i,j} = q_i^c k_j^{cT} + q_i^c k_j^{pT} + q_i^p k_j^{cT}
$$

即：
$$
\text{score}_{i,j} = q_i k_j^T + q_i R_{i,j}^T + R_{j,i} k_j^T
$$

**特点**：
- 解耦内容与位置；
- 最后几层加入绝对位置编码用于微调（称为 Decoder），但非 Seq2Seq 结构。

---

## 1.3 其他位置编码

### （1）CNN式：Zero Padding 泄露位置信息

ICLR 2020 论文《How Much Position Information Do Convolutional Neural Networks Encode?》发现：

- CNN 本身无显式位置编码；
- 但由于 **zero padding 的存在**，卷积核在边缘和中心的感受野不同，从而间接编码了位置信息。

**结论**：CNN 能捕捉位置信息，主要源于 padding 引起的边界效应。

### （2）复数式（Complex Order Embeddings）

出自 ICLR 2020 论文《Encoding word order in complex embeddings》。

每个词 $j$ 有三组参数向量：幅值 $r_j$、频率 $\omega_j$、相位 $\theta_j$，位置 $k$ 的编码为：

$$
\text{emb}_{j,k} = \left[ r_{j,d} \cdot e^{i(\omega_{j,d} k + \theta_{j,d})} \right]_{d=1}^D
$$

整个模型运行在**复数域**。

**特点**：
- 利用复数旋转表示位置变化；
- 本质上是一种隐式的相对位置编码；
- 实现复杂，未被主流采纳。

### （3）融合式（RoPE：Rotary Position Embedding）

见下节详述。

---

## 1.4 总结对比

| 类型 | 代表方法 | 是否可外推 | 是否需训练 | 特点 |
|------|----------|------------|------------|------|
| 绝对位置编码 | Learnable, Sinusoidal | 否（Learnable）/ 是（Sinusoidal） | Learnable需，Sinusoidal否 | 简单直观，但外推性差 |
| 相对位置编码 | RPR, T5, DeBERTa | 是 | 多数需 | 更符合语言特性，外推性强 |
| RoPE | RoFormer, LLaMA | 是（配合NTK等改进） | 否 | 结合绝对与相对优点 |
| ALiBi | ALiBi | 是 | 否 | 无需位置嵌入，偏置控制衰减 |

> **说明**：Sinusoidal 位置编码的内积可分解出 $m-n$ 项，因此也隐含相对位置信息。

---

## 2. 旋转位置编码（RoPE）

RoPE（Rotary Position Embedding）由苏剑林提出，用于 RoFormer 模型，后被 LLaMA、Palm 等采用。

### 核心思想

**通过旋转操作将绝对位置编码嵌入到 Query 和 Key 中，使得内积结果自然包含相对位置信息**。

### 数学推导

设 $q, k \in \mathbb{R}^d$，将它们按两两分组，每组视为复数：

$$
q_i = q_{2i} + i q_{2i+1},\quad k_i = k_{2i} + i k_{2i+1}
$$

定义位置 $m$ 的旋转操作：

$$
\tilde{q}_m = q \cdot e^{i m \theta_i},\quad \theta_i = 10000^{-2i/d}
$$

即旋转矩阵形式：

$$
\mathcal{R}_m = \begin{bmatrix}
\cos m\theta & -\sin m\theta \\
\sin m\theta & \cos m\theta
\end{bmatrix}
$$

则内积为：

$$
\langle \tilde{q}_m, \tilde{k}_n \rangle = \text{Re}\left[ \sum_{i=0}^{d/2-1} q_i k_i^* e^{i(m-n)\theta_i} \right]
$$

> **关键点**：**内积结果只依赖于相对位置 $m-n$**，实现了相对位置编码！

### 优势

- 无需修改 Attention 结构；
- 可预先计算旋转矩阵；
- 天然支持外推；
- 被证明等价于一种“频率调制”的相对编码。

---

## 3. ALiBi（Attention with Linear Biases）

出自论文《Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation》。

### 核心思想

**不在输入中添加位置编码，而是在注意力得分上直接加上一个与相对位置成线性关系的偏置**：

$$
\text{score}_{i,j} = q_i k_j^T - m \cdot |i - j|
$$

其中 $m$ 是头相关的衰减系数，通常设为 $m_h = 2^{-8(h+1)/H}$，$H$ 为头数。

### 特点

- 无需位置嵌入；
- 偏置矩阵固定，无需训练；
- 距离越远，得分越低，强制注意力关注局部；
- 外推性极强：训练长度 1024，测试可达 2048 甚至更长。

---

## 4. 长度外推问题

### 4.1 什么是长度外推问题？

指模型在**训练时使用较短序列（如 1024），在推理时处理更长序列（如 2048）时性能下降**的现象。

本质是：
1. 位置编码无法覆盖新位置；
2. 注意力分布熵增大，注意力更分散。

### 4.2 解决方法

#### （1）进制表示（Base-N Encoding）

将大整数用高进制表示，减少数值跨度。例如用 16 进制表示位置，可覆盖更大范围。

#### （2）直接外推（Extrapolation）

训练时预留高位维度（设为 0），推理时启用。但因未训练，效果差。

#### （3）线性插值（Linear Interpolation）

将位置 $k$ 映射到新范围：$k' = k \cdot \frac{L_{\text{test}}}{L_{\text{train}}}$。需微调以适应“拥挤”分布。

#### （4）NTK-aware Scaled RoPE

改进 RoPE 的频率：

$$
\theta_i = 10000^{-2i/d} \cdot \left( \frac{L_{\text{test}}}{L_{\text{train}}} \right)^{2i/d}
$$

使高频分量适应更长序列，显著提升外推性能。

> **说明**：此方法本质是“频率进制缩放”，与进制转换思想一致。

### 4.3 外推需解决的两个问题

1. **位置编码外推**：新位置要有合理表示；
2. **注意力分散**：长序列导致注意力稀释，需机制聚焦关键位置。

### 4.4 为何主流 LLM 少用 ALiBi？

1. RoPE + NTK 已足够好；
2. ALiBi 强制局部注意力，可能损害全局理解；
3. 多数模型优先保证训练长度内性能；
4. ALiBi 在 PPL 上表现好，但在复杂任务中未必占优。

---

## 参考资料

- [RoPE 原文](https://arxiv.org/abs/2104.09864)
- [ALiBi 论文](https://arxiv.org/abs/2211.05100)
- [T5 论文](https://arxiv.org/abs/1910.10683)
- [DeBERTa 论文](https://arxiv.org/abs/2006.03654)
- [苏剑林博客：RoPE 推导](https://spaces.ac.cn/archives/8265)
- [NTK-RoPE](https://blog.eleuther.ai/ntk-aware-scaled-rope/)
